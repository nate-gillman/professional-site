<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Using Fourier series, we build a neural network layer which learns categorical distributions that have a continuous structure.">
  <meta property="og:title" content="Fourier Head: Helping Large Language Models Learn Complex Probability Distributions"/>
  <meta property="og:description" content="Using Fourier series, we build a neural network layer which learns categorical distributions that have a continuous structure."/>
  <meta property="og:url" content="https://nategillman.com/fourier-head"/>
  <meta name="twitter:title" content="Fourier Head: Helping Large Language Models Learn Complex Probability Distributions">
  <meta name="twitter:description" content="Using Fourier series, we build a neural network layer which learns categorical distributions that have a continuous structure.">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Machine Learning, Generative Modeling, Fourier, Series, Fourier Series, Deep Learning, LLM, Large language model, continuity, continuous, Artificial Intelligence">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Fourier head</title>
  <link rel="icon" type="image/x-icon" href="fourier-head-static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="stylesheet" href="fourier-head-static/css/bulma.min.css">
  <link rel="stylesheet" href="fourier-head-static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="fourier-head-static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="fourier-head-static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="fourier-head-static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="fourier-head-static/js/fontawesome.all.min.js"></script>
  <script src="fourier-head-static/js/bulma-carousel.min.js"></script>
  <script src="fourier-head-static/js/bulma-slider.min.js"></script>
  <script src="fourier-head-static/js/index.js"></script>
  <script type="module" src="fourier-head-static/js/fourier-series-component.js"></script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.min.js"></script>
  <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.css"/>
  <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick-theme.css"/>
  <style>
    .glow {
      animation: glowing 1.5s infinite;
      border-radius: 50%;
    }
    @keyframes glowing {
      0% {
        box-shadow: 0 0 5px rgba(255, 165, 0, 0.5);
      }
      50% {
        box-shadow: 0 0 20px rgba(255, 165, 0, 1);
      }
      100% {
        box-shadow: 0 0 5px rgba(255, 165, 0, 0.5);
      }
    }
  </style>
  <style>
    .caption-container {
      margin: 0 auto;
    }
    
    @media (max-width: 768px) {
      .caption-container {
        max-width: 90%;
        min-width: 90%;
      }
    }
    
    @media (min-width: 769px) {
      .caption-container {
        max-width: 60%;
        min-width: 60%;
      }
    }
  </style>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><img src="fourier-head-static/images/favicon.ico" alt="image" style="width: 48px; height: 48px; vertical-align: text-top;"> Fourier Head:
              <br>Helping Large Language Models Learn<br>Complex Probability Distributions</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://nategillman.com/" target="_blank">Nate Gillman</a><sup>*</sup>,
              </span>
              <span class="author-block">
                <a href="https://dakshces.github.io/" target="_blank">Daksh Aggarwal</a><sup>*</sup>, 
              </span>
              <span class="author-block">
                <a href="https://tempoxylophone.github.io/" target="_blank">Michael Freeman</a>,
              </span>
              <span class="author-block">
                <a href="https://www.saurabhsingh.info" target="_blank">Saurabh Singh</a>,
              </span>
              <span class="author-block">
                <a href="https://chensun.me/" target="_blank">Chen Sun</a>
              </span>
            </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      Brown University, Google DeepMind
                      <br>
                      <div class="is-size-6 publication-authors">
                        <span class="author-block">
                          <sup>*</sup>Equal contribution
                        </span>
                      </div>
                      <a href="https://nategillman.com/fourier-head" target="_blank">ICLR 2025</a>
                    </span>
                  </div>


                  <div class="column has-text-centered">
                    <div class="publication-links">
                      
                      <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                        </a>
                      </span> -->

                      <!-- Supplementary PDF link -->
                      <!-- <span class="link-block">
                        <a href="scsc-static/pdfs/supplementary_material.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Supplementary</span>
                        </a>
                      </span> -->

                      <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2410.22269" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                      </span>

                      <!-- Twitter thread Link -->
                      <span class="link-block">
                        <a href="https://x.com/GillmanLab/status/1852015295470166477" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="fab fa-twitter"></i>
                        </span>
                        <span>Twitter Intro</span>
                        </a>
                    </span>

                      <!-- Github link -->
                      <span class="link-block">
                        <a href="https://github.com/nate-gillman/fourier-head" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                        </a>
                      </span>
            </div>
            <h2 class="subtitle has-text-centered">
              <br><br>
              <em>
                The Fourier head is a neural network layer which learns a <b>continuous</b> probability
                density function using Fourier series, and returns a <b>discrete</b> approximation of it.
              </em>
              <br><br>
              <em>
                Large language models are often adapted to model non-linguistic tokens.
                If these tokens have an underlying continuous structure, then replacing the linear
                classification head with the Fourier head can boost downstream performance.
              </em>
            </h2>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser is-light">
<div class="your-class">
  <div>
    <div width>
      <br>
      <h2 class="title is-4 has-text-centered">
        <b>Toy Example: <b><FONT COLOR="#62A0CA">Fourier Head</FONT></b> Learns a <b><FONT COLOR="#2CA02C">Square Wave</FONT></b></b>
      </h2>
      <fourier-series-component 
        data-source="fourier-head-static/js/data-square-wave.json">
      </fourier-series-component>
      <div class="caption-container">
        <h2 class="subtitle has-text-justified">
          We demonstrate how the Fourier head can learn a square wave.
          <b>As we increase the number of frequencies, 
          the <b><FONT COLOR="#62A0CA">Fourier head</FONT></b> does a better job approximating the <b><FONT COLOR="#2CA02C">square wave</FONT></b>,
          and it becomes less smooth.</b>
          This trend illustrates the Fourier Head Scaling Law.
          In this example, we consider a Fourier PDF with \(N=1,\dots,64\) frequencies,
          and \(128\) output dimensions.
        </h2>
        <br/>
      </div>
    </div>
  </div>
  <div>
    <div>
      <br>
      <h2 class="title is-4 has-text-centered">
        <b>Toy Example: <b><FONT COLOR="#62A0CA">Fourier Head</FONT></b> Learns a <b><FONT COLOR="#2CA02C">Gaussian Mixture Model</FONT></b></b>
      </h2>
      <fourier-series-component 
        data-source="fourier-head-static/js/data-mixture-of-gaussians-v1.json">
      </fourier-series-component>
      <div class="caption-container">
        <h2 class="subtitle has-text-justified">
          We demonstrate how the Fourier head can learn a GMM.
          <b>As we increase the number of frequencies, 
          the <b><FONT COLOR="#62A0CA">Fourier head</FONT></b> does a better job approximating the <b><FONT COLOR="#2CA02C">GMM</FONT></b>,
          and it becomes less smooth.</b>
          This trend illustrates the Fourier Head Scaling Law.
          In this example, we consider a Fourier PDF with \(N=1,\dots,64\) frequencies,
          and \(128\) output dimensions.
        </h2>
        <br/>
      </div>
    </div>
  </div>
  <div>
    <div>
      <br>
      <h2 class="title is-4 has-text-centered">
        <b>Toy Example: <b><FONT COLOR="#62A0CA">Fourier Head</FONT></b> Learns a <b><FONT COLOR="#2CA02C">Complicated Gaussian Mixture Model</FONT></b></b>
      </h2>
      <fourier-series-component 
        data-source="fourier-head-static/js/data-mixture-of-gaussians-v2.json">
      </fourier-series-component>
      <div class="caption-container">
        <h2 class="subtitle has-text-justified">
          We demonstrate how the Fourier head can learn a complicated GMM.
          <b>As we increase the number of frequencies, 
          the <b><FONT COLOR="#62A0CA">Fourier head</FONT></b> does a better job approximating the <b><FONT COLOR="#2CA02C">GMM</FONT></b>,
          and it becomes less smooth.</b>
          This trend illustrates the Fourier Head Scaling Law.
          In this example, we consider a Fourier PDF with \(N=1,\dots,64\) frequencies,
          and \(128\) output dimensions.
          <br>
        </h2>
        <br/>
      </div>
    </div>
  </div>
  <div>
    <div class="container is-max-desktop">
      <div class="hero-body">
        <br>
        <h2 class="title is-4 has-text-centered">
          Toy Example: <b><FONT COLOR="#62A0CA">Fourier head</FONT></b> outperforms the <b><FONT COLOR="#E06768">linear classification head</FONT></b>
        </h2>
        <h2 class="subtitle has-text-centered">
          <img src="fourier-head-static/images/toy_predicted_vs_true_gmm_2.png" alt="MY ALT TEXT"/>
        </h2>
        <h2 class="subtitle has-text-justified">
          As a low-dimensional demonstration, we task an MLP with learning to approximate a <b><FONT COLOR="#2CA02C">continuous bimodal density</FONT></b> using a categorical distribution 
          and a cross entropy objective.
          We observe that a standard <b><FONT COLOR="#E06768">linear classification head</FONT></b>  fails to distinguish between the two modes, 
          and overfits to high-frequency noise in the training set.
          In contrast, our proposed <b><FONT COLOR="#62A0CA">Fourier head</FONT></b> learns a smoother, more accurate categorical distribution.
          Our paper provides theoretical justification for the Fourier head, as well as empirical justification on a large scale imitation learning task,
          and a time series foundation model pretraining task.
        </h2>
      </div>
    </div>
  </div>
</div>
</section>

<script type="text/javascript">
  $(document).ready(function(){
    $('.your-class').slick({
      arrows: true,
      dots: true,
      infinite: true,
      slidesToShow: 1,
      slidesToScroll: 1,
      draggable: true,
      swipe: true,
      touchMove: true,
      autoplay: false,
      autoplaySpeed: 3000,
      adaptiveHeight: true,
    });
  });
</script>


<script>
  document.addEventListener("DOMContentLoaded", function() {
    const callback = function(mutationsList) {
    mutationsList.forEach(mutation => {
        if (mutation.type === 'childList') {
          const containers = document.querySelectorAll('.slick-dots');
          containers.forEach(container => {
            const buttons = container.querySelectorAll('button');
            buttons.forEach(button => {
              button.classList.add('glow');
            });
          });
        }
      });
    };
    const observer = new MutationObserver(callback);
    observer.observe(document.body, { childList: true, subtree: true });
  });
</script>

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            As the quality of large language models has improved, 
            there has been increased interest in using them to model non-linguistic tokens.
            For example, the Decision Transformer recasts agentic decision making as a sequence modeling problem, 
            using a decoder-only LLM to model the distribution over the discrete action space for an Atari agent.
            However, when adapting LLMs to non-linguistic domains, it remains unclear if softmax over discrete bins 
            captures the continuous structure of the tokens and the potentially complex distributions needed for high quality token generation.
            <b>We introduce a neural network layer, constructed using Fourier series, which we can easily substitute 
            for any linear layer if we want the outputs to have a more continuous structure.</b>
            We perform extensive analysis on synthetic datasets, as well as on large-scale decision making and time series forecasting tasks.
            We also provide theoretical evidence that this layer can better learn signal from data while ignoring high-frequency noise.
            All of our results support the effectiveness of our proposed Fourier head in scenarios where the 
            underlying data distribution has a natural continuous structure.
            For example, <b>the Fourier head improves a Decision Transformer agent's returns by 46% on the Atari 
            Seaquest game, and increases a state-of-the-art times series foundation model's forecasting performance by 3.5% across 20 benchmarks unseen during training.</b>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="hero teaser is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br>
      <h2 class="title is-4 has-text-centered">
        <b>Large-scale example #1: <b><FONT COLOR="#62A0CA">Fourier head</FONT></b> increases returns of an Atari agent by 46%.</b>
      </h2>
      <br>
      <h2 class="subtitle has-text-centered">
        <img src="fourier-head-static/images/atari_graph_varying_freqs.png" alt="MY ALT TEXT"/>
      </h2>
      <h2 class="subtitle has-text-justified">
        <b>We replace the linear classification head in the Decision Transformer agent with a Fourier head.</b>
        We find that the Decision Transformer agent with the <b><FONT COLOR="#62A0CA">Fourier head</FONT></b> 
        achieves larger returns than the baseline agent with the <b><FONT COLOR="#E06768">linear head</FONT></b>, 
        so long as the Fourier head has sufficiently many frequencies.
        We also find that the Fourier agent's returns have lower variance.
        For normalized returns, higher is better; for smoothness, lower is better.
      </h2>
      <h2 class="subtitle has-text-centered">
        <img src="fourier-head-static/images/atari_graph_PMFs-65.png" alt="MY ALT TEXT"/>
      </h2>
      <h2 class="subtitle has-text-justified">
        We present next-action distribution examples for two different Decision Transformer agents--one trained to predict the next action 
        using a <b><FONT COLOR="#E06768">linear classification head</FONT></b> as in the original implementation, 
        and the other using our proposed <b><FONT COLOR="#62A0CA">Fourier head</FONT></b>.
        We can see that the <b><FONT COLOR="#62A0CA">Fourier</FONT></b> agent produces a "clump" of actions that is semantically meaningful.
        Namely, this agent almost certainly wants to shoot in the down right or right direction, presumably because there is a submarine in that direction.
        In contrast, the <b><FONT COLOR="#E06768">linear</FONT></b> agent's next-action distribution doesn't clearly depict a strategy, 
        and assigns higher likelihoods to incorrect actions.
        <b>Because the Fourier head outputs a smoother PMF, it concentrates more probability mass near the correct action, resulting in better returns.</b>
      </h2>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br>
      <h2 class="title is-4 has-text-centered">
        Large-scale example #2: <b><FONT COLOR="#62A0CA">Fourier head</FONT></b> improves time series forecasting accuracy.
      </h2>
      <br>
      <h2 class="subtitle has-text-centered">
        <img src="fourier-head-static/images/chronos_results.png" alt="MY ALT TEXT" style="width: 70%">
      </h2>
      <h2 class="subtitle has-text-justified">
        <b>We replace the linear classification head in the Chronos time series foundation model with a Fourier head.</b>
        We find that the model with the <b><FONT COLOR="#62A0CA">Fourier head</FONT></b> 
        achieves better forecasting accuracy than the baseline model with the <b><FONT COLOR="#E06768">linear head</FONT></b>.
      </h2>
      <h2 class="subtitle has-text-centered">
        <img src="fourier-head-static/images/chronos-PMF.png" alt="MY ALT TEXT" style="width: 85%"/>
      </h2>
      <h2 class="subtitle has-text-justified">
        We present next-value distribution examples for two different time series foundation models--one trained to predict the 
        next quantized value using a <b><FONT COLOR="#E06768">linear classification head</FONT></b>, and the other using 
        our proposed <b><FONT COLOR="#62A0CA">Fourier head</FONT></b>.
        We can see that the <b><FONT COLOR="#62A0CA">Fourier head</FONT></b> model produces a smoother next-token distribution
        than the <b><FONT COLOR="#E06768">linear head</FONT></b>.
      </h2>
    </div>
  </div>
</section>




<section class="hero teaser is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br>
      <h2 class="title is-4 has-text-centered">
        Overview of Procedure: Forward Pass Through the Fourier Head
      </h2>
      <br>
      <h2 class="subtitle has-text-centered">
        <img src="fourier-head-static/images/fourier_head_algo.png" alt="MY ALT TEXT"/>
      </h2>
      <h2 class="subtitle has-text-justified">
        At a high level, the Fourier head inputs \( x \in \mathbb{R}^n \), 
        uses a linear layer to learn the coefficients for a Fourier series with \( N \) frequencies over \([-1, 1]\), 
        and quantizes the interval \([-1, 1]\) into \( m \) equal bins.
        Then, the Fourier head evaluates the learned Fourier PDF at those \( m \) bin center points, 
        and returns those \( m \) likelihoods as a categorical distribution.
        <b>We include a PyTorch implementation on our github repository, as well as many examples of its usage.</b>
      </h2>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre>
<code>@inproceedings{
  gillman2025fourier,
  title={Fourier Head: Helping Large Language Models Learn Complex Probability Distributions},
  author={Nate Gillman and Daksh Aggarwal and Michael Freeman and Chen Sun},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=4hPwLg7zD3}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Default Statcounter code for Fourier Head project page
https://nategillman.com/fourier-head -->
<script type="text/javascript">
  var sc_project=13050285; 
  var sc_invisible=1; 
  var sc_security="486448f6"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js"
  async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img
  class="statcounter"
  src="https://c.statcounter.com/13050285/0/486448f6/1/"
  alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->

  </body>
  </html>
