<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="We demonstrate that automatic self-correction functions are a promising solution to stabilizing self-consuming generative model training and avoiding model collapse.">
  <meta property="og:title" content="Self-Correcting Self-Consuming Loops for Generative Model Training"/>
  <meta property="og:description" content="We demonstrate that automatic self-correction functions are a promising solution to stabilizing self-consuming generative model training and avoiding model collapse."/>
  <meta property="og:url" content="https://nategillman.com/sc-sc.html"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="scsc-static/image/your_banner_image.png" /> -->
  <!-- <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="scsc-static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Machine Learning, Generative Modeling, Self-Consuming Loops, Data Contamination, Deep Learning, Artificial Intelligence, Human Motion Synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Self-Correcting Self-Consuming</title>
  <link rel="icon" type="image/x-icon" href="scsc-static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="scsc-static/css/bulma.min.css">
  <link rel="stylesheet" href="scsc-static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="scsc-static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="scsc-static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="scsc-static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="scsc-static/js/fontawesome.all.min.js"></script>
  <script src="scsc-static/js/bulma-carousel.min.js"></script>
  <script src="scsc-static/js/bulma-slider.min.js"></script>
  <script src="scsc-static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Self-Correcting Self-Consuming Loops<br>For Generative Model Training</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://nategillman.com/" target="_blank">Nate Gillman</a>,
              </span>
              <span class="author-block">
                <a href="https://tempoxylophone.github.io/" target="_blank">Michael Freeman</a>,
              </span>
              <span class="author-block">
                <a href="https://dakshces.github.io/" target="_blank">Daksh Aggarwal</a>, 
              </span>
              <span class="author-block">
                <a target="_blank">Chia-Hong Hsu</a>,
              </span>
              <span class="author-block">
                <a href="https://calvinyluo.com/about.html" target="_blank">Calvin Luo</a>,
              </span>
              <span class="author-block">
                <a target="_blank">Yonglong Tian</a>,
              </span>
              <span class="author-block">
                <a href="https://chensun.me/" target="_blank">Chen Sun</a>
              </span>

            </div>

                  <div class="is-size-5 publication-authors">
                    <!-- <span class="author-block">Institution Name<br>Conferance name and year</span> -->
                    <span class="author-block">Brown University, Google Research</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      
                      <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                        </a>
                      </span> -->

                      <!-- Supplementary PDF link -->
                      <!-- <span class="link-block">
                        <a href="scsc-static/pdfs/supplementary_material.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Supplementary</span>
                        </a>
                      </span> -->

                      <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://nategillman.com/sc-sc.html" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv (coming soon)</span>
                      </a>
                      </span>

                      <!-- Github link -->
                      <span class="link-block">
                        <a href="https://nategillman.com/sc-sc.html" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-github"></i>
                        </span>
                        <span>Code (coming soon)</span>
                        </a>
                      </span>


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="scsc-static/videos/motion_null_cropped.mp4"
        type="video/mp4">
      </video>
      <br><br>
      <h2 class="subtitle has-text-centered">
        What happens after iteratively training a text-conditioned generative model for human motion synthesis for 50 generations? 
        We simulate a self-consuming loop by creating synthetic data with the latest generative model, and mixing them with the 
        original data to continue training the next generative model. We observe that by self-correcting the synthetic data 
        with a physics simulator, the model can successfully avoid 
        <b><FONT COLOR="#00A300">collapse</FONT></b> and generate 
        <b><FONT COLOR="#0476D0">high-quality human motion</FONT></b>. Our paper 
        provides theoretical and empirical justification for the self-correcting self-consuming loop.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained 
            on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation 
            learning, using synthetic data for generative model training creates “self-consuming loops” which may lead to training 
            instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative 
            model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data 
            point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We 
            then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), 
            and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of 
            self-correcting self-consuming loops on the challenging human motion synthesis task, and observe that it successfully avoids 
            model collapse, even when the ratio of synthetic data to real data is as high as 100%.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br><br>
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Effect of Correction Strength</h2>
      </div>
      <h2 class="subtitle has-text-centered">
        Our empirical results demonstrate that increasing our proposed correction strength hyperparameter
      improves performance and stability after self-consuming iterations.
      </h2>
      <h2 class="subtitle has-text-centered">
        <img src="scsc-static/images/gaussian_toy_summary.png" alt="MY ALT TEXT"/>
      </h2>
    </div>
  </div>
</section>



<section class="hero teaser is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br><br>
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Examples: Synthesized Human Motion</h2>
      </div>
      <h2 class="subtitle has-text-centered">
        How does our proposed self-correction operation affect the self-consuming loop for the human motion generation task?
        We can see that the 
        <b><FONT COLOR="#00A300">self-consuming</FONT></b> model produces a motion 
        which doesn't reflect the prompt description.
        Additionally, this motion produces frames that represent physically impossible human motions--notice when the human 
        suddenly snaps to a position where the leg penetrates the ground plane.
        These negative artifacts do not exist in the motions synthesized from the 
        <b><FONT COLOR="#E55B13">baseline</FONT></b> model or 
        <b><FONT COLOR="#0476D0">self-consuming with self-correction</FONT></b> model.
      </h2>
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="scsc-static/videos/motion_glitch_cropped.mp4"
        type="video/mp4">
      </video>
      <br>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br><br>
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Examples: Synthesized Human Motion</h2>
      </div>
      <h2 class="subtitle has-text-centered">
        We can see that the 
        <b><FONT COLOR="#00A300">self-consuming</FONT></b> model outputs random motions that slide the figure to the right
        and have no relation to the text prompt.
        Additionally, the human rotates their forearm unnaturally and forcefully.
        In contrast, the <b><FONT COLOR="#E55B13">baseline</FONT></b> and <b><FONT COLOR="#0476D0">self-consuming with self-correction</FONT></b>
        models both generate motions with accurately embody the prompt.
      </h2>
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="scsc-static/videos/motion_spurious_cropped.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section>




<!-- Images -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <br>
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Quantitative Analysis</h2>
      </div>
      <br>
      <h2 class="subtitle has-text-centered">
        Our quantitative analysis demonstrates that the 
        <b><FONT COLOR="#0476D0">self-consuming with self-correction</FONT></b> model outperforms the 
        <b><FONT COLOR="#00A300">self-consuming</FONT></b> model, in that it more stably 
        converges to a better FID score, and more quickly. 
        When the dataset size is smaller (top) we can see that the self-consuming model has a flat Matching score, as well as diverging FID and 
        Diversity scores, indicating model collapse. 
        When the dataset size is larger (bottom) there is less 
        collapse in the self-consuming model, although the variance of the FID score between generations is worse, which indicates training instability.
        And in this case, the self-consuming with self-correction is competitive with the <b><FONT COLOR="#E55B13">baseline</FONT></b>
        even after 50 generations.
      </h2>
      <div class="item">
        <img src="scsc-static/images/0064_100_1k_iterative_finetuning.png" alt="MY ALT TEXT"/>
      </div>
      <br>
       <div class="item">
        <img src="scsc-static/images/40k_25_iterative_finetuning.png" alt="MY ALT TEXT"/>
      </div>
      <br>
    </div>
  </div>
</section>
<!-- End images --> 






<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br><br>
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Example: Synthesized Human Motion, Failure Case</h2>
      </div>
      <br>
      <h2 class="subtitle has-text-centered">
        We highlight a failure case of the <b><FONT COLOR="#0476D0">self-consuming loop with self-correction</FONT></b>. 
        When the dataset size is small (n=64) the self-consuming loop tends to suffer from a lack of diverity.
        The video demonstrates how different samples from the same prompt result in nearly identical motions.
      </h2>
      <br>
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="scsc-static/videos/motion_diversity_failure_cropped.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section>



<section class="hero teaser is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br><br>
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Example: Retaining Diversity With Larger Dataset</h2>
      </div>
      <br>
      <h2 class="subtitle has-text-centered">
        However, we find that this issue is resolved when using a larger dataset (n=2794).
        With the same prompt, the <b><FONT COLOR="#0476D0">self-consuming with self-correction</FONT></b> model 
        generates diverse and correct motions.
      </h2>
      <br>
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="scsc-static/videos/motion_diversity_success_self_consuming_with_correction_cropped.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section>





<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>(coming soon)</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
<!-- Default Statcounter code for SCSC
https://cs.brown.edu/people/ngillman//sc-sc.html -->
<script type="text/javascript">
  var sc_project=12966006; 
  var sc_invisible=1; 
  var sc_security="1efea264"; 
</script>
<script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async>
</script>
<noscript>
  <div class="statcounter">
    <a title="site stats" href="https://statcounter.com/" target="_blank">
      <img class="statcounter" src="https://c.statcounter.com/12966006/0/1efea264/1/" alt="site stats" referrerPolicy="no-referrer-when-downgrade">
    </a>
  </div>
</noscript>
<!-- End of Statcounter Code -->

  </body>
  </html>
